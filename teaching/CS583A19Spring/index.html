<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
  /*
   * Copyright 2013 Christophe-Marie Duquesne <chmd@chmd.fr>
   *
   * CSS for making a resume with pandoc. Inspired by moderncv.
   *
   * This CSS document is delivered to you under the CC BY-SA 3.0 License.
   * https://creativecommons.org/licenses/by-sa/3.0/deed.en_US
   */
  
  /* Whole document */
  body {
      font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
      width: 800px;
      margin: auto;
      background: #FFFFFF;
      padding: 10px 10px 10px 10px;
  }
  
  /* Title of the resume */
  h1 {
      font-size: 55px;
      color: #757575;
      text-align:center;
      margin-bottom:15px;
  }
  h1:hover {
      background-color: #757575;
      color: #FFFFFF;
      text-shadow: 1px 1px 1px #333;
  }
  
  /* Titles of categories */
  h2 {
      /* This is called "sectioncolor" in the ConTeXt stylesheet. */
      color: #397249;
  }
  /* There is a bar just before each category */
  h2:before {
      content: "";
      display: inline-block;
      margin-right:1%;
      width: 16%;
      height: 10px;
      /* This is called "rulecolor" in the ConTeXt stylesheet. */
      background-color: #9CB770;
  }
  h2:hover {
      background-color: #397249;
      color: #FFFFFF;
      text-shadow: 1px 1px 1px #333;
  }
  
  /* Definitions */
  dt {
      float: left;
      clear: left;
      width: 17%;
      /*font-weight: bold;*/
  }
  dd {
      margin-left: 17%;
  }
  p {
      margin-top:0;
      margin-bottom:7px;
  }
  
  /* Blockquotes */
  blockquote {
      text-align: center
  }
  
  /* Links */
  a {
      text-decoration: none;
      color: #397249;
  }
  a:hover, a:active {
      background-color: #397249;
      color: #FFFFFF;
      text-decoration: none;
      text-shadow: 1px 1px 1px #333;
  }
  
  /* Horizontal separators */
  hr {
      color: #A6A6A6;
  }
  
  table {
      width: 100%;
  }
  </style>
</head>
<body>
<h1 id="cs583-deep-learning">CS583: Deep Learning</h1>
<blockquote>
<p>Instructor: Shusen Wang</p>
</blockquote>
<h2 id="description">Description</h2>
<p><strong>Meeting Time:</strong></p>
<ul>
<li><p>Thursday, 6:30 - 9:00 PM, Peirce Complex 116</p></li>
<li><p><strong>The classes on these dates are canceled: Jan 31</strong></p></li>
</ul>
<p><strong>Office Hours:</strong></p>
<ul>
<li><p>Thursday, 3:00 - 5:00 PM, North Building 205</p></li>
<li><p><strong>The office hours on these dates are canceled: Jan 31, Feb 28</strong></p></li>
</ul>
<p><strong>Contact the Instructor:</strong></p>
<ul>
<li><p>For questions regarding grading, talk to the instructor during office hours or send him emails.</p></li>
<li><p>For any other questions, come during the office hours; the instructor will NOT reply such emails.</p></li>
</ul>
<p><strong>Prerequisite:</strong></p>
<ul>
<li><p>Elementary linear algebra, e.g., matrix multiplication, eigenvalue decomposition, and matrix norms.</p></li>
<li><p>Elementary calculus, e.g., convex function, differentiation of scalar functions, first derivative, and second derivative.</p></li>
<li><p>Python programming (especially the Numpy library) and Jupyter Notebook.</p></li>
</ul>
<p><strong>Goal:</strong> This is a practical course; the students will be able to use DL methods for solving real-world ML, CV, and NLP problems.</p>
<h2 id="schedule">Schedule</h2>
<ul>
<li><p>Jan 24, Lecture 1</p>
<p>-- Fundamental ML problems</p>
<p>-- Regression</p></li>
<li><p>Jan 24, <strong>Homework 0</strong> is assigned.</p>
<p>-- <a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM0/HM.pdf">Click here for the assignment</a></p>
<p>-- Submission is not required.</p>
<p>-- Deadline: finish it before the 1st Quiz. (Otherwise, you will probably fail.)</p></li>
<li><p>Jan 24, <strong>Homework 1</strong> is assigned (available on Canvas).</p>
<p>-- Submission: submit to Canvas.</p></li>
<li><p>Jan 31, <strong>CANCELED</strong> due to the instructor's conference traveling</p></li>
<li><p>Feb 7, Lecture 2</p>
<p>-- Regression (cont.)</p>
<p>-- Classification</p></li>
<li><p>Feb 14, Lecture 3</p>
<p>-- Classification (cont.)</p>
<p>-- Dimensionality reduction</p></li>
<li><p>Feb 14, <strong>Homework 2</strong> is assigned (available on Canvas).</p>
<p>-- Submission: submit to Canvas.</p></li>
<li><p>Feb 21, Lecture 4</p></li>
<li><p>Feb 24, <strong>Deadline for Homework 1</strong></p></li>
<li><p>Feb 28, <strong>Quiz 1</strong> (coverage: linear algebra, optimization, and ML basics).</p></li>
<li><p>Mar 7, <strong>Deadline for project proposal</strong></p>
<p>-- Submission: submit to Canvas.</p></li>
<li><p>Mar 7, Lecture 5</p>
<p>-- Neural network basics</p>
<p>-- Keras</p></li>
<li><p>Mar 7, <strong>Homework 3</strong> is assigned</p>
<p>-- Available at the course's repo [<a href="https://github.com/wangshusen/CS583A-2019Spring.git">click here</a>]</p>
<p>-- Submission: submit to Canvas.</p></li>
<li><p>Mar 14, Lecture 6</p>
<p>-- Neural network basics (cont.)</p>
<p>-- Convolutional neural networks</p></li>
<li><p>Mar 14, <strong>Deadline for Homework 2</strong></p></li>
<li><p>Mar 21, Spring Break, no class</p></li>
<li><p>Mar 28, Lecture 7</p>
<p>-- Convolutional neural networks (cont.)</p></li>
<li><p>Apr 4, Lecture 8</p>
<p>-- Convolutional neural networks (cont.)</p></li>
<li><p>Apr 4, <strong>Homework 4</strong> is assigned</p>
<p>-- Available at the course's repo [<a href="https://github.com/wangshusen/CS583A-2019Spring.git">click here</a>]</p>
<p>-- Submission: submit to Canvas.</p></li>
<li><p>Apr 7, <strong>Deadline for Homework 3</strong></p></li>
<li><p>Apr 11, Lecture 9</p>
<p>-- Autoencoders</p></li>
<li><p>Apr 18, Lecture 10</p>
<p>-- Recurrent neural networks</p></li>
<li><p>Apr 25, Lecture 11</p>
<p>-- Recurrent neural networks (cont.)</p></li>
<li><p>May 2, Lecture 12</p>
<p>-- Recurrent neural networks (cont.)</p></li>
<li><p>May 4, <strong>Deadline for Homework 4</strong></p></li>
</ul>
<h2 id="syllabus-and-slides">Syllabus and Slides</h2>
<ol style="list-style-type: decimal">
<li><p><strong>Machine learning basics.</strong> This part briefly introduces the fundamental ML problems-- regression, classification, dimensionality reduction, and clustering-- and the traditional ML models and numerical algorithms for solving the problems.</p>
<p>-- ML basics. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/1_ML_Basics.pdf">slides</a>]</p>
<p>-- Regression. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/2_Regression_1.pdf">slides-1</a>] [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/2_Regression_2.pdf">slides-2</a>]</p>
<p>-- Classification. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/3_Classification_1.pdf">slides-1</a>][<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/3_Classification_2.pdf">slides-2</a>] [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/3_Classification_3.pdf">slides-3</a>] [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/3_Classification_4.pdf">slides-4</a>]</p>
<p>-- Regularizations. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/4_Optimization.pdf">slides-1</a>][<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/4_Regularizations.pdf">slides-2</a>]</p>
<p>-- Dimensionality reduction. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/5_SVD.pdf">slides-1</a>] [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/5_PCA.pdf">slides-2</a>] [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/5_MatrixComputations.pdf">slides-3</a>]</p></li>
<li><p><strong>Neural network basics.</strong> This part covers the multilayer perceptron, backpropagation, and deep learning libraries, with focus on Keras.</p>
<p>-- Multilayer perceptron and backpropagation. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/6_NeuralNet_1.pdf">slides</a>]</p>
<p>-- Keras. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/6_NeuralNet_2.pdf">slides</a>]</p>
<p>-- Further reading: [<a href="https://adl1995.github.io/an-overview-of-activation-functions-used-in-neural-networks.html">activation functions</a>][<a href="https://isaacchanghau.github.io/post/loss_functions/">loss functions</a>] [<a href="https://isaacchanghau.github.io/post/weight_initialization/">parameter initialization</a>][<a href="https://isaacchanghau.github.io/post/parameters_update/">optimization algorithms</a>]</p></li>
<li><p><strong>Convolutional neural networks (CNNs).</strong> This part is focused on CNNs and its application to computer vision problems.</p>
<p>-- CNN basics. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/7_CNN_1.pdf">slides-1</a>][<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/7_CNN_2.pdf">slides-2</a>] [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/7_CNN_3.pdf">slides-3</a>]</p>
<p>-- Advanced topics on CNNs. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/7_CNN_4.pdf">slides</a>]</p>
<p>-- Popular CNN architectures. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/7_CNN_5.pdf">slides</a>]</p>
<p>-- Further reading: [style transfer (Section 8.1, Chollet's book)][visualize CNN (Section 5.4, Chollet's book)]</p></li>
<li><p><strong>Autoencoders.</strong> This part introduces autoencoders for dimensionality reduction and image generation.</p>
<p>-- Autoencoder for dimensionality reduction.</p>
<p>-- Variational Autoencoders (VAEs) for image generation.</p></li>
<li><p><strong>More image generation methods.</strong> <strong>(Optional, depending on the progress.)</strong> This part covers two image generation approaches in addition to VAE. The training of neural networks takes fixed images (X) as inputs and optimize w.r.t. the weights (W). Conversely, one can fix the network's weights (W) and optimize w.r.t. the input X. In this way, new images are generated to maximize (or minimize) some function; the application includes attacking neural networks and deep dream. Another very different approach is the generative adversarial network (GAN).</p>
<p>-- Attack neural networks.</p>
<p>-- Deep dream.</p>
<p>-- Generative adversarial network (GAN).</p></li>
<li><p><strong>Recurrent neural networks (RNNs).</strong> This part introduces RNNs and its applications in natural language processing (NLP).</p>
<p>-- Text processing. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/10_RNN_1.pdf">slides</a>]</p>
<p>-- RNN basics and LSTM. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/10_RNN_2.pdf">slides</a>][<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">reference</a>]</p>
<p>-- Text generation. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/10_RNN_3.pdf">slides</a>]</p>
<p>-- Machine translation. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/10_RNN_4.pdf">slides</a>]</p>
<p>-- Attention. [<a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/slides/10_RNN_5.pdf">slides</a>][<a href="https://distill.pub/2016/augmented-rnns/">reference-1</a>] [<a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html">reference-2</a>]</p>
<p>-- Further reading: [<a href="http://www.aclweb.org/anthology/D14-1162">GloVe: Global Vectors for Word Representation</a>][<a href="https://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization.pdf">Neural Word Embedding as Implicit Matrix Factorization</a>]</p></li>
<li><p><strong>Recommender system.</strong> <strong>(Optional, depending on the progress.)</strong> This part is focused on the collaborative filtering approach to recommendation based on the user-item rating data. This part covers matrix completion methods and neural network approaches.</p>
<p>-- Recommender system.</p></li>
</ol>
<h2 id="project">Project</h2>
<p>Every student must participate in one <a href="https://www.kaggle.com/competitions">Kaggle competition</a>.</p>
<ul>
<li><p><strong>Details</strong>: <a href="https://github.com/wangshusen/CS583A-2019Spring/blob/master/project/Project/proj.pdf">click here</a></p></li>
<li><p><strong>Deadlines</strong></p>
<ul>
<li><p>Submit a proposal to Canvas before Mar 7.</p></li>
<li><p>(Not required.) Submit a preliminary version to Canvas before April 21th if you want to compete for the presentation and bonus.</p></li>
<li><p>Submit the final version to Canvas before the final. (May be extended.)</p></li>
</ul></li>
<li><p><strong>Submissions</strong>: Put your source code and report to a Github repository and submit the links to Canvas.</p></li>
<li><p><strong>Teamwork policy</strong>: You had better work on your own project. Teamwork (up to 3 students) is allowed if the competition has a heavy workload; the workload and team size will be considered in the grading.</p></li>
</ul>
<h2 id="textbooks">Textbooks</h2>
<p><strong>Required</strong>:</p>
<ul>
<li>Francois Chollet. Deep learning with Python. Manning Publications Co., 2017. (Available online.)</li>
</ul>
<p><strong>Recommended</strong>:</p>
<ul>
<li><p>Y. Nesterov. Introductory Lectures on Convex Optimization Book. Springer, 2013. (Available online.)</p></li>
<li><p>D. S. Watkins. Fundamentals of Matrix Computations. John Wiley &amp; Sons, 2004.</p></li>
<li><p>I. Goodfellow, Y. Bengio, A. Courville, Y. Bengio. Deep learning. MIT press, 2016. (Available online.)</p></li>
<li><p>M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of machine learning. MIT press, 2012.</p></li>
<li><p>J. Friedman, T. Hastie, and R. Tibshirani. The elements of statistical learning. Springer series in statistics, 2001. (Available online.)</p></li>
</ul>
<h2 id="grading-policy">Grading Policy</h2>
<p><strong>Grading percentages</strong>:</p>
<ul>
<li><p>Homework 50%</p></li>
<li><p>Final 15%</p></li>
<li><p>Project 20%</p></li>
<li><p>Quizzes 15%</p></li>
<li><p>Bonus (up to 10%)</p></li>
</ul>
<p><strong>Late penalty</strong>:</p>
<ul>
<li><p>Late submissions of assignments or project document for whatever reason will be punished. 1% of the score of an assignment/project will be deducted per day. For example, if an assignment is submitted 15 days and 1 minute later than the deadline (counted as 16 days) and it gets a grade of 95%, then the score after the deduction will be: 95% - 16% = 79%.</p></li>
<li><p>June 1 is the firm deadline for all the homework. Submissions later than June 1 will not be graded.</p></li>
</ul>
</body>
</html>
